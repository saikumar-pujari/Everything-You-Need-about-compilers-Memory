Introduction
  In old computers, CPU and memory were about the same speed.
  Today, CPUs are way faster than memory â†’ that gap causes delays.
  Fixing slow memory needs better hardware (RAM design, controllers, caches, DMA).
  The paper focuses on explaining caches and memory to help programmers write faster code.

1ï¸âƒ£ What is Memory?
Memory = where the CPU stores and retrieves data while running programs.
Think of it like your desk workspace:
The CPU = you (the worker).
Memory = your desk where you keep tools/papers youâ€™re working with.
Disk = the storage cabinet (slow but large).
ğŸ’¡ Rule #1: Closer = faster, Smaller = faster, Bigger = slower.

âš™ï¸ 2ï¸âƒ£ Memory Hierarchy (The â€œSpeed Pyramidâ€)
Level	          Type	      Size  	    Speed	          Analogy
CPU Registers	  Small  	    Few bytes	  âš¡ Super fast	  In your hand
L1 Cache	      Tiny	      KBs	        âš¡âš¡ Fast	    On your desk
L2 Cache  	    Medium	    MBs	        âš¡ Fast        	In your drawer
L3 Cache	      Bigger	    MBs	        âš¡ Okay	        In your room
RAM	            Large	      GBs	        ğŸ¢ Slower        In another room
SSD	            Huge	      TBs        	ğŸ¢ğŸ¢ Slow      	In your car
HDD	            Bigger	    TBs	        ğŸ¢ğŸ¢ğŸ¢ Very slow	  Across town
Network/Cloud	  Infinite   	â€”          	ğŸŒ Slowest	    Other city

ğŸ‘‰ CPU works fastest with data closest to it.

âš™ï¸ 3ï¸âƒ£ CPU Registers
These are tiny storage cells inside the CPU that hold the most active variables.
Example: while doing a + b, both a and b are loaded into registers.
Speed: ~1 nanosecond (instant).
Limitation: only a few exist â€” so compiler decides what to keep there.

âš¡ 4ï¸âƒ£ CPU Cache (L1, L2, L3)
ğŸ§© What It Is:
Cache is small, super-fast memory between CPU and RAM.
It keeps recently used data so CPU doesnâ€™t wait for RAM.
ğŸ’¡ Example:
Youâ€™re reading a book.
You keep the current page on your desk (cache).
The rest is in the shelf (RAM).
If you flip between nearby pages â†’ fast.
If you jump to random chapters â†’ slower (cache miss).
ğŸ§  Important:
L1 Cache: per-core, smallest and fastest.
L2 Cache: shared per-core, a bit bigger.
L3 Cache: shared among all cores, slower but huge.
Each cache â€œtalksâ€ in cache lines (usually 64 bytes chunks).
ğŸš¨ Cache Miss:
When data is not in cache â†’ CPU fetches from RAM (100x slower).
So, you want to reuse nearby data.

ğŸ’¾ 5ï¸âƒ£ RAM (Main Memory)
RAM (DRAM) is where most active data lives.
Itâ€™s fast but not instant â€” ~100 ns latency.
RAM is made of tiny capacitors that need refreshing (hence slower).
Accessing sequential memory is fast (burst mode). Random access is slow.
ğŸ’¡ Example:
Looping through int arr[1000000] sequentially = âœ… fast.
Jumping around arr[random()] = ğŸ¢ slow.

âš™ï¸ 6ï¸âƒ£ Virtual Memory
The OS gives each process its own private memory space (virtual addresses).
Real physical memory is mapped behind the scenes.
The OS and MMU (Memory Management Unit) translate addresses â†’ physical.
This allows:
Process isolation (no program can touch anotherâ€™s memory).
Easier memory allocation.
Paging and swapping (use disk when RAM full).
ğŸ’¡ Example:
Your program thinks it has 4 GB, but physically it could be spread all over the RAM.

ğŸ§© 7ï¸âƒ£ Paging & Swapping
Memory is divided into pages (usually 4 KB each).
When RAM fills up, OS moves least-used pages to disk (swap).
If your program uses swapped memory â†’ super slow.
ğŸ’¡ Example:
If you alt-tab between many Chrome tabs, youâ€™ll see lag â€” thatâ€™s swapping.

ğŸ§  8ï¸âƒ£ NUMA (Non-Uniform Memory Access)
In multi-CPU systems, each CPU has its own local RAM.
Accessing local memory is fast; accessing another CPUâ€™s memory is slower.
So threads should work on data near their CPU.
ğŸ’¡ Example:
Imagine each friend (CPU) has their own fridge (RAM).
If you always take from your friendâ€™s fridge instead of yours, itâ€™s slower.

âš™ï¸ 9ï¸âƒ£ Memory Bandwidth vs Latency
Bandwidth: how much data per second you can move.
ğŸ§  Like a highwayâ€™s width.
Latency: how long one trip takes.
ğŸ•’ Like travel time from start to destination.
CPUs hide latency with prefetching, caching, and out-of-order execution.
ğŸ’¡ Example:
A 4-lane road (high bandwidth) doesnâ€™t help if your destination is far (high latency).

ğŸ§© 10ï¸âƒ£ Cache Coherency (for Multi-core CPUs)
Each CPU core has its own cache â†’ same variable may exist in multiple caches.
Coherency protocols (like MESI) ensure all cores see the same value.
ğŸ’¡ Example:
Two people editing the same Google Doc â€” both should see updates instantly.
